# prefix-tuning-gpt

![rinna-icon](./rinna.png)

This repository demonstrates how to conduct [prefix-tuning](https://arxiv.org/abs/2101.00190) with GPT/[GPT-NeoX](https://github.com/EleutherAI/gpt-neox) models and to do inference with trained prefix weights.

The example training code `src/prefix_tuning_example.py` trains prefix-tuning weights that encourage a GPT/GPT-NeoX model to end every generated sentence with a smiling face emoji ğŸ˜ƒ. 100 documents from [Japanese CC-100](http://data.statmt.org/cc-100) are used as sample data for training/validation, and the data is placed at `data/sample_data.jsonl`.

The code has been verified on [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small). The trained weights has been released in [the same model hub page](https://huggingface.co/rinna/japanese-gpt-neox-small).

[Deepspeed](https://www.deepspeed.ai/) is used for accelerating training and for reducing memory use when needed.

| Table of Contents |
|-|
| [Update log](#update-log) |
| [Use example](#use-example) |
| [Research repos by rinna](#research-repos-by-rinna) |
| [License](#license) |

---

## Update log

* 2023/03/22 Adapt the code to [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small)'s recent [update](https://huggingface.co/rinna/japanese-gpt-neox-small/commit/d93a6e4cfd29b7a4cccc068c24f4afab9c218c45). Notice that a newer version of transformer is required now.
* 2022/10/11 Use prefix wrapper to allow for applying deepspeed to both a base model and a prefix encoder.
* 2022/09/05 Release.

---

## Use example

### Install dependency

* Run the following command
    ~~~
    pip install -r requirements.txt
    ~~~

### Prefix-tuning `japanese-gpt-neox-small` on 1 GPU

* Run the following commands
    ~~~
    cd src/
    deepspeed --include localhost:0 --module prefix_tuning_example \
        --model_type gpt-neox \
        --pretrained_model_dir rinna/japanese-gpt-neox-small \
        --data_filepath ../data/sample_data.jsonl \
        --train_data_size 1000 \
        --dev_data_size 10 \
        --batch_size 4 \
        --max_lr 0.0001 \
        --deepspeed \
        --deepspeed_config ds_config.json \
        --save_name gpt-neox-small_suffix \
        --save_model
    ~~~

* The best checkpoint will be saved at `prefix-tuning-gpt/data/model/{FILENAME}.best.checkpoint`.

* *NOTE:* For larger models, feel free to explore arguments such as `-fp16` to reduce memory use. See the arguments in `src/prefix_tuning_example.py` for details.

### Inference

* Run the following command using the previously trained prefix weight checkpoint or using the trained prefix weight file `smileface_suffix.task0.weight` from [our Huggingface model hub page](https://huggingface.co/rinna/japanese-gpt-neox-small/tree/main).
    ~~~
    CUDA_VISIBLE_DEVICES=0 python -m prefix_inference \
        --model_type gpt-neox \
        --pretrained_model_dir rinna/japanese-gpt-neox-small \
        --prefix_checkpoint_path ../data/model/{FILENAME}.best.checkpoint
    ~~~

* Compare the samples generated from the above command with those generated without the trained prefix weights by removing the `--prefix_checkpoint_path` argument.
    ~~~
    CUDA_VISIBLE_DEVICES=0 python -m prefix_inference \
        --model_type gpt-neox \
        --pretrained_model_dir rinna/japanese-gpt-neox-small
    ~~~
    You should be able to see the difference such as in the following generated samples.
    
    3 samples without the prefix weights:
    > 1. ã€Œãã£ã¨ãã‚Œã¯çµ¶å¯¾é–“é•ã£ã¦ãªã„ã­ã€‚ ã‚ãŸã—ã«ã¯5ã‹å›½èªã«4ã¤ã®å¤–å›½èªã®æ„å‘³ãªã‚“ã¦ã‚ã‹ã‚‰ãªã„ã€‚ ã§ã‚‚ã€ã¨ã‚Šã‚ãˆãšã“ã®ç°¡å˜ãªè‹±æ–‡ãŒã©ã‚“ãªæ„å‘³ã‚’æŒã¤ã®ã‹çŸ¥ã‚ŠãŸã„ã‚ˆã­!ã€
    > 2. 25åˆ†é ƒã«å…¬åœ’ã«ç€ã„ã¦ã€ãƒ™ãƒ³ãƒã«åº§ã£ã¦å¾…ã£ã¦ã„ã‚‹ã¨ã€ã¾ãŸã—ã¦ã‚‚Så…ˆç”Ÿã‹ã‚‰é€£çµ¡ãŒå…¥ã‚Šã¾ã—ãŸã€‚ ç¢ºã‹ã€åˆå¾Œã®ç¤¼æ‹ã®æ™‚ã«è‡ªåˆ†ã®æŒã£ã¦ããŸãŠå¼å½“ã‚’é£Ÿã¹ãŸè¨˜æ†¶ãŒé®®æ˜ã«æ®‹ã£ã¦ã„ã¾ã™ã€‚ å¾Œã§ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã§æ¤œç´¢ã—ãŸã‚‰ã€Så…ˆç”Ÿã®ãƒ–ãƒ­ã‚°ã«é£›ã³ã¾ã—ãŸã€‚ ä»Šæ—¥ã®æ™©ã”ã¯ã‚“ã¯ç„¼ããƒŠã‚¹ã‚’ä½œã£ã¦ã¿ã¾ã—ãŸ! * ä¸Šã®å†™çœŸã¯æ˜¨æ—¥ã®æœç„¼ã‘ã§ã™ã€‚
    > 3. CTã§æ­¯å½¢ãŒã§ãã¦ã€ãã®å¾Œã•ã‚‰ã«ãã®æ­¯å½¢ãŒå†ã³å™›ã‚ã‚‹ã‚ˆã†ã«ãªã‚‹ã®ã¯ã€ä½•ãŒåŸå› ã ã‚ã†? è™«æ­¯ã«ãªã£ãŸåŸå› ã‚‚ã€å£è‡­ã‹ãª? ãã‚Œã¨ã‚‚æ­¯å‘¨ç—…ã‹ãª? æ­¯çŸ³ãŒã¨ã‚Œã‚‹ã¾ã§ã€ã€ã€ã‚‚ã†ã¡ã‚‡ã£ã¨ã‹ã‹ã‚Šãã†ã€‚ å­ä¾›ã®è™«æ­¯ã£ã¦ã€ãªã‹ãªã‹æ²»ã‚‰ãªã„ã§ã™ã‚ˆã­ã€‚è¦ªå…„å¼Ÿã§ä½•åº¦ã‹ã€‚ å­ä¾›ã®æ­¯æ ¹ã¯ã€è¦ªã®ã‚‚ã®ã«ãªã‚Šã¾ã™ã€‚ ãã—ã¦è‡ªåˆ†ã®ã‚‚ã®ã ã£ãŸã‚Šã€çŸ¥ã‚‰ãªã„é–“ã«æŠœã„ãŸã‚Šã—ã€ç”Ÿãˆã¦ããŸã‚Šã‚‚ã—ã¾ã™ã€‚ å¤§äººã«ãªã£ã¦è¦ªã‹ã‚‰ã¿ãŸå ´åˆã¯ã€ç™½ã„æ­¯ã«å¤‰ã‚ã£ã¦ãã¦ã€é‡‘å±ã®ã‚ˆã†ãƒ¼ã§ã‚‚æ‚ªããªãã€è¦ªã‹ã‚‰ã®ã‚€ã—æ­¯ã®å¿ƒé…ã¯ãªã„ã§ã™ã‚ˆã­ã€‚
    
    3 samples with the prefix weights:
    > 1. â€»æµ·å¤–ãƒ–ãƒ©ãƒ³ãƒ‰å“ã®å ´åˆã¯ã€è¿”å“ãƒ»è¿”é‡‘ç­‰ã¯ãŠå—ã‘è‡´ã—ã‹ã­ã¾ã™ã®ã§äºˆã‚ã”äº†æ‰¿é¡˜ã„ã¾ã™ã€‚ â€» å•†å“ç™ºé€å¾Œã€ãŠå®¢æ§˜ã¸å•†å“è¿”é€å®Œäº†ã¾ã§ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚’é‡è¦–ã™ã‚‹æ–¹ã¯æµ·å¤–ãƒ–ãƒ©ãƒ³ãƒ‰å“ã‚’å…ˆã«é€ã‚Šä»˜ã‘ã•ã›ã¦é ‚ã ã‚±ãƒ¼ã‚¹ãŒã”ã–ã„ã¾ã™ã€‚ ğŸ˜ƒ
    > 2. ç§ã¯éå»ã«æŒã£ã¦ã„ãŸä¸å‹•ç”£ã‚’ã€ä¸­å¤ä½å®…ã¨ã—ã¦å£²å´ã—ã¦ã„ã¾ã—ãŸãŒã€ãã®å¾Œã®ç§ã®çŠ¶æ³ã¯ã©ã†ã ã£ãŸã®ã§ã—ã‚‡ã†ã‹? ğŸ˜ƒ çµæœã¨ã—ã¦ã¯ã€æŠ•è³‡ç‰©ä»¶ã¨ã—ã¦å£²å´ã‚’è€ƒãˆã¦ã„ã¾ã™ãŒã€ä»Šã¾ã§ã®ç›¸å ´ã‚‚èª­ã‚“ã§ã„ãŸã ã‘ã°ã‚ã‹ã‚‹ã¨æ€ã„ã¾ã™ã€‚ ğŸ˜ƒ ä»Šã¾ã§ã€ç‰©ä»¶ã«å¯¾ã—ã¦ã®æŠ•è³‡ã¯éå¸¸ã«æ§ãˆã‚ã«ã—ã¦ããŸã®ã§ã™ãŒã€ä»Šå›ã®ææ¡ˆã‚’èª­ã‚“ã§ã€å®Ÿéš›ã«ç‰©ä»¶ã‚’è³¼å…¥ã™ã‚‹éš›ã«ã¯ãã¡ã‚“ã¨ç¢ºèªã‚’ã—ã‚ˆã†ã¨æ€ã„ã¾ã™ã€‚ ğŸ˜ƒ
    > 3. ã“ã®å†™çœŸé›†ã®è¡¨ç´™ã‚’ã“ã®å°ç´™ã«ã—ã¦ã„ã‚‹ä½œå®¶ã•ã‚“ã¯ã€ã¾ã‚‹ã§èª°ã‹ã®æŒ‡ç¤ºã‚’å—ã‘ã¦è¡Œå‹•ã—ã¦ã„ã‚‹äººç‰©ã®ã‚ˆã†ã«è¦‹ãˆã‚‹ã€ã¨ã„ã†ã®ãŒã€ã“ã®ä½œå“ã‚’ã‚„ã¶ã«ã‚‰ã‚“ã ã€Œæ®ºã—å±‹é›†å›£ã€ã®æã„ã¦ã„ã‚‹ä½œå“ã§ã‚ã‚‹ã‚ˆã†ã«æ€ ã„ã¾ã™ã€‚ ğŸ˜ƒ

---

## Research repos by rinna

| Link to repo |
|-|
| [japanese-pretrained-models](https://github.com/rinnakk/japanese-pretrained-models) |
| [japanese-clip](https://github.com/rinnakk/japanese-clip) |
| [prefix-tuning-gpt](https://github.com/rinnakk/prefix-tuning-gpt) |

---

## License

[The Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0)
